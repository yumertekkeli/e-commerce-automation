{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Fulfilment By Amazon (FBA) E-commerce Product Portfolio Management Project** #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pipeline I'll present the steps i've performed during my internship to optimize efficiency for product portfolio management with e-commerce data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **PART I:  FBA Data** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data provided by Amazon consists three reports; in order Business Performance Report, Sales & Traffic Report, Inventory report for a list of Key Performance Indicators(KPI) for the business. All reports are collected in **daily periods**  to have time-series of products for further analysis purposes. List of the KPI's and report purposes in more detail: \n",
    "\n",
    "**Business Performance Report:**\n",
    "\n",
    "A general view of products operating in the website and list of KPI's to determine position of products in Amazons web page and listings.\n",
    "- ASIN (Amazon Standard Identification Number): Unique Identifiers of the products.\n",
    "- Title: The product name or description.\n",
    "- Brand Name: The brand associated with each product.\n",
    "- Average Customer Review and Number of Reviews: Indicators of customer satisfaction and product popularity.\n",
    "- Sales Rank: A measure of a product's sales performance relative to others in its category. Eventhough the exact algorithm of this KPI is well hidden by Amazon it is correlated with sales, stock performance, customer reviews, page interactions.\n",
    "\n",
    "**Sales and Traffic Report:**\n",
    "\n",
    "The report consists insights for product sales performance, customer engagement, page views, revenue from sales for both Business To Business (B2B) and Business To Customer (B2C). \n",
    "\n",
    "- ASIN: Unique identifiers for the products.\n",
    "- Title: The product name or description.\n",
    "- SKU (Stock Keeping Unit): A unique identifier used for inventory management.\n",
    "- Sessions - Total: The total number of visits to the product's page.\n",
    "- Session Percentage - Total: The percentage of sessions relative to Sessions - Total.\n",
    "- Page Views - Total: The total number of times the product page was viewed.\n",
    "- Units Ordered: The total number of units sold.\n",
    "- Ordered Product Sales: Total revenue from sales.\n",
    "\n",
    "**Amazon FBA Inventory Report:**\n",
    "\n",
    "This report is for understanding the inventory levels, stock management and ensuring product quality before shipment is performed.\n",
    "\n",
    "- SKU: Unique identifiers for products.\n",
    "- Fulfillment Channel SKU: Indicates the fulfillment method (either FBA or by business directly).\n",
    "- ASIN : Unique identifiers for products.\n",
    "- Condition Type: Describes the state of the products (new, used, etc.).\n",
    "- Warehouse Condition code: This code assigned by Amazon, indicates whether a product is considered sellable or unsellable in their - warehouses before being shipped to customers.\n",
    "- Quantity Available: The stock level of each item.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List of Packages** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Merging daily reports and updating**\n",
    "Amazon reports named default for **Sales and Traffic & Business Performance Reports**: The date is in month-day-year and in .csv format, with respect to the *date* report downloaded (e.g. BusinessReport-7-24-79.csv). For **Amazon FBA Inventory Report** in similar way but in .txt format (e.g. Amazon-fulfilled+Inventory+07-24-1979.txt). For the purpose of having a concated complete daily time series dataset for each reports, defined functions below to run live when there is new reports available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "excel_dir = \"your directory\"\n",
    "\n",
    "def sales_rank_data_update(excel_dir):\n",
    "\n",
    "    'This function is responsible for updating the sales rank data from a directory of CSV files.'\n",
    "\n",
    "    # Create a dictionary to store the DataFrames\n",
    "    dfs = {}\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(excel_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_file = os.path.join(excel_dir, filename)\n",
    "            \n",
    "            # Extract the date from the filename    \n",
    "            date_str = filename.replace('BusinessReport-', '').replace('.csv', '')\n",
    "            date = datetime.strptime(date_str, '%m-%d-%y').date()\n",
    "            \n",
    "            # Create the DataFrame key in the desired format\n",
    "            df_key = f'df_{date.strftime(\"%m-%d-%y\")}'\n",
    "            \n",
    "            # Load the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Add the 'Date_Requested' column to the DataFrame\n",
    "            df['Date_Requested'] = date.strftime('%m-%d-%y') # changed\n",
    "\n",
    "            df['Date_Requested'] = pd.to_datetime(df['Date_Requested']).dt.date #changed\n",
    "            # Add the DataFrame to the dictionary with the desired key\n",
    "            dfs[df_key] = df\n",
    "\n",
    "    for key, df in dfs.items():\n",
    "        print(f\"DataFrame {key}: {df.shape}\")\n",
    "\n",
    "    # Concatenate all the DataFrames in the dictionary\n",
    "    sales_rank_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    sales_rank_combined = sales_rank_combined.sort_values('Date_Requested').reset_index(drop = True)\n",
    "    return sales_rank_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_dir = \"your directory\"\n",
    "\n",
    "def sales_traffic_data_update(excel_dir):\n",
    "\n",
    "    'This function is responsible for updating the sales and traffic data from a directory of CSV files.'\n",
    "\n",
    "    # Create a dictionary to store the DataFrames\n",
    "    dfs = {}\n",
    "\n",
    "    # Loop through all files in the directory\n",
    "    for filename in os.listdir(excel_dir):\n",
    "        if filename.endswith('.csv'):\n",
    "            csv_file = os.path.join(excel_dir, filename)\n",
    "            \n",
    "            # Extract the date from the filename\n",
    "            date_str = filename.replace('BusinessReport-', '').replace('.csv', '')\n",
    "            date = datetime.strptime(date_str, '%m-%d-%y').date()\n",
    "            \n",
    "            # Create the DataFrame key in the desired format\n",
    "            df_key = f'df_{date.strftime(\"%m-%d-%y\")}'\n",
    "            \n",
    "            # Load the CSV file into a pandas DataFrame\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            # Add the 'Date_Requested' column to the DataFrame\n",
    "            df['Date_Requested'] = date.strftime('%m-%d-%y')\n",
    "\n",
    "            df['Date_Requested'] = pd.to_datetime(df['Date_Requested']).dt.date\n",
    "            \n",
    "            # Add the DataFrame to the dictionary with the desired key\n",
    "            dfs[df_key] = df\n",
    "\n",
    "    for key, df in dfs.items():\n",
    "        print(f\"DataFrame {key}: {df.shape}\")\n",
    "\n",
    "    # Concatenate all the DataFrames in the dictionary.\n",
    "    sales_traffic_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    sales_traffic_combined['Date_Requested'] = pd.to_datetime(sales_traffic_combined['Date_Requested'])\n",
    "\n",
    "    # If you want to specifically format it later for display, you can use:\n",
    "    formatted_dates = sales_traffic_combined['Date_Requested']\n",
    "\n",
    "    sales_traffic_combined = sales_traffic_combined.sort_values('Date_Requested').reset_index(drop = True)\n",
    "\n",
    "    return sales_traffic_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_dir = \"your directory\"\n",
    "\n",
    "def daily_inventory_update(inventory_dir):\n",
    "    dfs = {}\n",
    "    for filename in os.listdir(inventory_dir):\n",
    "        \n",
    "        if filename.endswith('.txt'):\n",
    "             csv_file = os.path.join(inventory_dir, filename)\n",
    "\n",
    "             date_str = filename.replace('Amazon-fulfilled+Inventory+', '').replace('.txt', '')\n",
    "             date_str = date_str.strip()\n",
    "\n",
    "             date = datetime.strptime(date_str, '%m-%d-%Y').date()\n",
    "\n",
    "             \n",
    "             # Create the DataFrame key in the desired format\n",
    "             df_key = f'df_{date.strftime(\"%m-%d-%Y\")}'\n",
    "\n",
    "             # Load the txt file into a pandas DataFrame\n",
    "             df = pd.read_csv(csv_file,delimiter= '\\t')\n",
    "             df['Date_Requested'] = date.strftime('%m-%d-%Y')\n",
    "             df['Date_Requested'] = pd.to_datetime(df['Date_Requested'])\n",
    "\n",
    "             # Add the DataFrame to the dictionary with the desired key\n",
    "             dfs[df_key] = df\n",
    "\n",
    "    for key, df in dfs.items():\n",
    "        print(f\"DataFrame {key}: {df.shape}\")\n",
    "\n",
    "    \n",
    "    # Concatenate all the DataFrames in the dictionary\n",
    "    inventory_combined = pd.concat(dfs.values(), ignore_index=True)\n",
    "    #renaming the columns\n",
    "    inventory_combined = inventory_combined.rename(columns={'seller-sku': 'SKU','fulfillment-channel-sku': 'FNSKU','asin': 'ASIN','Warehouse-Condition-code':'Warehouse_Code','Quantity Available':'Available_(FBA)'}) \n",
    "    \n",
    "    \n",
    "    #drop list for columns \n",
    "    columns_drop_inventory = ['condition-type']\n",
    "    inventory_combined = inventory_combined.drop(columns_drop_inventory, axis=1)\n",
    "\n",
    "    \n",
    "    columns_to_convert = ['ASIN', 'SKU', 'FNSKU','Warehouse_Code']\n",
    "    inventory_combined[columns_to_convert] = inventory_combined[columns_to_convert].astype(str)\n",
    "    \n",
    "    #reordering dataframe for a better output\n",
    "    new_order = ['Date_Requested', 'SKU', 'FNSKU', 'ASIN','Warehouse_Code','Available_(FBA)']\n",
    "    inventory_combined = inventory_combined[new_order]\n",
    "    \n",
    "    # Rename the columns\n",
    "    inventory_combined.rename(columns={\n",
    "        'Date_Requested': 'date',\n",
    "        'SKU': 'sku',\n",
    "        'FNSKU': 'fnsku',\n",
    "        'ASIN': 'asin',\n",
    "        'Warehouse_Code': 'warehouse_code',\n",
    "        'Available_(FBA)': 'fba_available'\n",
    "    }, inplace=True)\n",
    "\n",
    "    #In case there are ill posed sku numbers available in reports you can include these lines in your code.\n",
    "    #e.g.  amzn1.wdsku.v4.X8JkLmnoPqR4T1S7BfGhIjKl, amzn1.wdsku.v4.Z5WnOpQrStU8V2X3YjKlMrNo\n",
    "\n",
    "    #ill posed sku's or any other SKU or ASIN you'd like to include\n",
    "    #skus_to_drop = ['amzn1.wdsku.v4.Z5WnOpQrStU8V2X3YjKlMrNo','amzn1.wdsku.v4.X8JkLmnoPqR4T1S7BfGhIjKl']\n",
    "    #inventory_combined = inventory_combined[~inventory_combined['sku'].isin(skus_to_drop)]\n",
    "    \n",
    "    inventory_combined = inventory_combined[inventory_combined['warehouse_code'] != 'UNSELLABLE']\n",
    "    \n",
    "    return inventory_combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cleaning Datasets**\n",
    "In this section performing a cleaning step for the KPI's in order to exclude any inconsistencies prior performing the analysis and any further implementations. These inconsistencies and noise often occured from outdated products appearing in reports, ill posed unique identifiers (duplicate unique identifiers), false sales data (e.g. there are no total sales but appears as units ordered or the other way around), products without Sales Rank, products without title (may not be live yet in Amazons Page). In order to not face these inconsistencies performed a cleaning step for each report individually.\n",
    "\n",
    "In addition we've extracted daily prices through total sales and unites ordered from Sales and Traffic reports. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_rank_cleaning(df):\n",
    "    \"\"\"This function performs cleaning and preprocessing on the Sales Rank column of a given DataFrame (df).\"\"\"\n",
    "\n",
    "    title_colname = 'Title'\n",
    "    sales_rank_colname = 'Sales Rank'\n",
    "    \n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(['Featured Offer (Buy Box) Percentage', \n",
    "                   'Featured Offer (Buy Box) Percentage - B2B'], axis=1)\n",
    "\n",
    "    # Convert the 'Sales Rank' column to integer\n",
    "    df[sales_rank_colname] = df[sales_rank_colname].str.replace(',', '').astype(int)\n",
    "\n",
    "    df['Date_Requested'] = pd.to_datetime(df['Date_Requested'])\n",
    "    # Copy df and mask the values to omit\n",
    "    non_clean_df = df.copy()\n",
    "\n",
    "    # Create a boolean mask for rows where the title column is empty\n",
    "    title_mask = df[title_colname].str.strip().eq('')\n",
    "\n",
    "    # Create a boolean mask for rows where the sales rank column is 0\n",
    "    sales_rank_mask = (df[sales_rank_colname] == 0)\n",
    "\n",
    "    # Find the number of rows with empty titles\n",
    "    num_empty_title_rows = df[title_mask].shape[0]\n",
    "\n",
    "    # Find the number of rows with a title but no sales rank\n",
    "    num_zero_sales_rank_rows = df[~title_mask & sales_rank_mask].shape[0]\n",
    "\n",
    "    # Drop the rows that match the title mask\n",
    "    df = df.drop(df[title_mask].index)\n",
    "\n",
    "    # Drop the rows that have a title but no sales rank\n",
    "    df = df.drop(df[sales_rank_mask].index)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # Find the total number of rows removed\n",
    "    num_rows_removed = num_empty_title_rows + num_zero_sales_rank_rows\n",
    "    \n",
    "    # Print the number of rows with no title, rows with no sales rank, and the total rows removed\n",
    "    print(f\"{num_empty_title_rows} products have no {title_colname} (not Live)\")\n",
    "    print(f\"{num_zero_sales_rank_rows} products have a {sales_rank_colname} of 0 (Live but no sales rank yet)\")\n",
    "    print(f\"{num_rows_removed} products omitted from live inventory. \"\n",
    "          f\"{df.shape[0]} products remaining for the analysis from {non_clean_df.shape[0]} products.\")\n",
    "\n",
    "    # Rename columns to lowercase\n",
    "    df.rename(columns={\n",
    "        'ASIN': 'asin',\n",
    "        'Title': 'title',\n",
    "        'Brand Name': 'brand',\n",
    "        'Sales Rank': 'sales_rank',\n",
    "        'Number of Customer Reviews': 'num_customer_reviews',\n",
    "        'Average Customer Review': 'customer_review_avg',\n",
    "        'Date_Requested': 'date',\n",
    "    }, inplace=True)\n",
    "\n",
    "    new_order = ['date','asin','title','brand','sales_rank','num_customer_reviews','customer_review_avg']\n",
    "    df = df[new_order]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sales_and_traffic_cleaning(df):\n",
    "\n",
    "    'This function performs cleaning and preprocessing on the Ordered Product Sales and traffic-related columns of a given DataFrame (df).'\n",
    "\n",
    "\n",
    "    df = df.rename(columns={'(Child) ASIN': 'ASIN','Ordered Product Sales':'Ordered Product Sales ($)'})\n",
    "    columns_drop_sales_traffic = ['Sessions - Total - B2B', 'Session Percentage - Total', 'Page Views - Total - B2B', 'Page Views Percentage - Total','Page Views Percentage - Total - B2B',\n",
    "                                'Featured Offer (Buy Box) Percentage','Featured Offer (Buy Box) Percentage - B2B','Units Ordered - B2B', 'Unit Session Percentage','Unit Session Percentage - B2B',\n",
    "                                'Ordered Product Sales - B2B','Total Order Items - B2B','Session Percentage - Total - B2B']\n",
    "\n",
    "    df = df.drop(columns_drop_sales_traffic, axis=1)\n",
    "\n",
    "\n",
    "    df['Ordered Product Sales ($)'] = df['Ordered Product Sales ($)'].str.replace('$', '')\n",
    "    df['Ordered Product Sales ($)'] = df['Ordered Product Sales ($)'].str.replace(',', '').astype(float)\n",
    " \n",
    "\n",
    "    sessions_convert = ['Sessions - Total','Page Views - Total']\n",
    "    df[sessions_convert] = df[sessions_convert].replace(',', '', regex=True).apply(pd.to_numeric, errors='coerce')\n",
    "    df[sessions_convert] = df[sessions_convert].astype(int)\n",
    "\n",
    "    # Mask the rows where 'Ordered Product Sales' is 0 but 'Units Ordered' is not 0\n",
    "    mask1 = (df['Ordered Product Sales ($)'] == 0) & (df['Units Ordered'] != 0)\n",
    "\n",
    "    # Mask the rows where 'Ordered Product Sales' is 0 but 'Total Order Items' is not 0\n",
    "    mask2 = (df['Ordered Product Sales ($)'] == 0) & (df['Total Order Items'] != 0)\n",
    "\n",
    "    # Combine the two masks\n",
    "    mask = mask1 | mask2\n",
    "    dropped_products_count = len(df[mask])\n",
    "\n",
    "    # Drop the rows that match the mask\n",
    "    df = df.loc[~mask]\n",
    "\n",
    "    # Convert 'Date_Requested' from YYYY-MM-DD to DD-MM-YY\n",
    "    df['Date_Requested'] = pd.to_datetime(df['Date_Requested'])\n",
    "    df['Sale Price'] = (df['Ordered Product Sales ($)'] / df['Units Ordered'].astype(float)).round(2)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    # Print a report\n",
    "    print(f\"Dropped {dropped_products_count} products that had units ordered but no sales.\")\n",
    "\n",
    "    columns_to_convert = ['ASIN', 'Title', 'SKU','(Parent) ASIN']\n",
    "    df[columns_to_convert] = df[columns_to_convert].astype(str)\n",
    "        \n",
    "    new_order = ['Date_Requested','(Parent) ASIN','ASIN','SKU','Title','Sale Price','Units Ordered', 'Ordered Product Sales ($)','Total Order Items','Sessions - Total',\n",
    "       'Page Views - Total']\n",
    "    df = df[new_order]    \n",
    "\n",
    "    # Rename columns to lowercase\n",
    "    df.rename(columns={\n",
    "    'ASIN': 'asin',\n",
    "    'Title': 'title',\n",
    "    '(Parent) ASIN' : 'parent_asin',\n",
    "    'SKU':'sku',\n",
    "    'Sale Price':'price',\n",
    "    'Units Ordered':'units_ordered',\n",
    "    'Total Order Items':'total_order_items',\n",
    "    'Sessions - Total':'sessions',\n",
    "    'Page Views - Total':'page_views',\n",
    "    'Ordered Product Sales ($)':'sales_usd',\n",
    "    'Date_Requested': 'date',\n",
    "}, inplace=True)\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inventory_clean(df):\n",
    "\n",
    "    wrong_skus = []\n",
    "\n",
    "    for asin in df['asin'].unique():\n",
    "        last_date = df['date'].max()\n",
    "        asin_df = df[(df['asin']== asin ) & (df['date']==last_date)].sort_values('fba_available').reset_index(drop = True)\n",
    "        if len(asin_df) > 1:\n",
    "            if asin_df['fba_available'].iloc[-1] > 0:\n",
    "                correct_asin = asin_df['asin'].iloc[-1]\n",
    "                correct_sku = asin_df['sku'].iloc[-1]\n",
    "                other_skus = asin_df[(asin_df['asin'] == correct_asin) & \n",
    "                                    (asin_df['sku'] != correct_sku)]['sku']\n",
    "                wrong_skus.extend(other_skus.tolist())\n",
    "\n",
    "        \n",
    "    df = df[~df['sku'].isin(wrong_skus)].reset_index(drop = True)\n",
    "\n",
    "    print(\"Wrong SKUs:\", wrong_skus)\n",
    "    print(\"Amount of SKUs dropped:\", len(wrong_skus))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting Sale Prices  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sale_price_add(df1, df2):\n",
    "\n",
    "    df2['price'] = np.nan  # Start with NaN to allow filling\n",
    "    if 'price' in df1.columns:\n",
    "        common_asins = set(df1['asin'].unique()) & set(df2['asin'].unique())\n",
    "        for asin in common_asins:\n",
    "            for date_requested in (set(df1[df1['asin'] == asin]['date'].unique()) & set(df2[df2['asin'] == asin]['date'].unique())):\n",
    "                df2.loc[(df2['asin'] == asin) & (df2['date'] == date_requested), 'price'] = df1.loc[(df1['asin'] == asin) & (df1['date'] == date_requested), 'price'].values[0]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Fill NaN values both forward and backward\n",
    "    for asin in df2['asin'].unique():\n",
    "        asin_mask = df2['asin'] == asin\n",
    "        \n",
    "        # Forward fill then backward fill for the current asin\n",
    "        df2.loc[asin_mask, 'price'] = df2.loc[asin_mask, 'price'].ffill()\n",
    "        df2.loc[asin_mask, 'price'] = df2.loc[asin_mask, 'price'].bfill()\n",
    "    \n",
    "    # Convert Sale Price to float\n",
    "    df2['price'] = df2['price'].fillna(0)\n",
    "    df2['price'] = df2['price'].astype(float)\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # **PART II:  Amazon KPI Tracker and Inventory Tracker** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take, for example, a small or large company that uses Amazon's platform to make sales, without mentioning the various reports offered across markets with a large number of products and tracking the performance of those products either directly or manually.CSV files would be a very ineffective and time-consuming method for the business to use. In order to address this issue, we collected all relevant data for product performance in Part I. In this section, we will create trackers to automate portfolio management activites.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sales Rank Tracker**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fasten up the process also created cleaning functions to trim dataset for given parameters for the tracker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sr_alert_clean(df, ma_period, days_req,ub,lb):\n",
    "\n",
    "    'This function performs cleaning and preprocessing on the input DataFrame (df) to prepare it for the sales rank alert analysis.'\n",
    "\n",
    "    # Make a copy of the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Get the most recent date\n",
    "    most_recent_date = pd.to_datetime(df['date'].max())\n",
    "    \n",
    "    # Calculate the start date for the required data\n",
    "    start_date = most_recent_date - pd.Timedelta(days=ma_period + days_req)\n",
    "    \n",
    "    # Filter the DataFrame to include only the required number of days\n",
    "    df_copy = df_copy[df_copy['date'] > start_date]\n",
    "    \n",
    "    # Create a list to store the ASINs that don't have enough data points\n",
    "    insufficient_data_asins = []\n",
    "    \n",
    "    # Filtering and storing step\n",
    "    for asin in df['asin'].unique():\n",
    "        asin_data = df_copy[df_copy['asin'] == asin]\n",
    "        if len(asin_data) < ma_period + days_req:\n",
    "            insufficient_data_asins.append(asin)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    # Drop the ASINs that don't have enough data points\n",
    "    clean_df = df_copy[~df_copy['asin'].isin(insufficient_data_asins)]\n",
    "    # print a report of insufficient_data_asins -> as a notification in the end? \n",
    "    \n",
    "    if  len(insufficient_data_asins) > 0 :\n",
    "        print(\"ASINs with insufficient data that were not included in the analysis:\")\n",
    "        for asin in insufficient_data_asins:\n",
    "                print(asin)\n",
    "    \n",
    "    print(\"Overall Report:\")\n",
    "    excluded = len(insufficient_data_asins)\n",
    "    print(f\"ASINs excluded from analysis: {excluded}\")\n",
    "    return clean_df\n",
    "\n",
    "def sales_rank_alert(df, ma_period, days_req, ub, lb):\n",
    "\n",
    "    'This function performs the sales_rank alert analysis on the input DataFrame (df).'\n",
    "    \n",
    "    'Parameters:'\n",
    "    'df (pandas.DataFrame): The input DataFrame containing the sales_rank data.'\n",
    "    'ma_period (int): The number of days to use for the moving average calculation.'\n",
    "    'days_req (int): The number of days for which the sales_rank alert analysis is required.'\n",
    "    'ub (float): The upper bound percentage threshold for the sales_rank alert.'\n",
    "    'lb (float): The lower bound percentage threshold for the sales_rank alert.'\n",
    "\n",
    "    'Returns:'\n",
    "    'pandas.DataFrame: The DataFrame with the alert_status column added, indicating whether the sales_rank is gaining, losing, or neutral.'\n",
    "\n",
    "    winners = []\n",
    "    neutrals = []\n",
    "    losers = []\n",
    "\n",
    "    most_recent_date = pd.to_datetime(df['date'].max())\n",
    "    \n",
    "    clean_df = sr_alert_clean(df, ma_period, days_req,ub,lb)\n",
    "    \n",
    "    clean_df['alert_status'] = ''\n",
    "    weekly_report = pd.DataFrame()\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for asin in clean_df['asin'].unique():\n",
    "        # Filters the main database with unique ASIN name\n",
    "        asin_df = clean_df[clean_df['asin'] == asin].reset_index(drop=True)\n",
    "\n",
    "        rolling_mean = np.zeros(days_req)\n",
    "        upperbound = np.zeros(days_req)\n",
    "        lowerbound = np.zeros(days_req)\n",
    "\n",
    "        for i in range(0, days_req):\n",
    "            rolling_mean[i] = asin_df['sales_rank'].iloc[i:ma_period+i].mean()\n",
    "\n",
    "            # Calculate the lower and upper bounds based on the rolling mean\n",
    "            upperbound[i] = (1 + ub) * rolling_mean[i]\n",
    "            lowerbound[i] = (1 - lb) * rolling_mean[i]\n",
    "            if asin_df['sales_rank'].iloc[(ma_period - days_req + 1) + i:(ma_period+1)+i].mean() > lowerbound[i] and asin_df['sales_rank'].iloc[(ma_period - days_req + 1) + i:(ma_period+1)+i].mean() < upperbound[i]:\n",
    "                        asin_df.loc[(ma_period+i), 'alert_status'] = 'Neutral'\n",
    "                        neutrals.append(asin)\n",
    "            elif asin_df['sales_rank'].iloc[(ma_period - days_req + 1) + i:(ma_period+1)+i].mean() > upperbound[i]:\n",
    "                        asin_df.loc[(ma_period+i), 'alert_status'] = 'Losing Rank'\n",
    "                        losers.append(asin)\n",
    "            elif asin_df['sales_rank'].iloc[(ma_period - days_req + 1) + i:(ma_period+1)+i].mean() < lowerbound[i]:\n",
    "                        asin_df.loc[(ma_period+i), 'alert_status'] = 'Gaining Rank'\n",
    "                        winners.append(asin)\n",
    "        # Append the updated asin_df to the list\n",
    "        merged_df = pd.concat([merged_df, asin_df.tail(days_req)], ignore_index=True)\n",
    "        weekly_report = merged_df[merged_df['date'] == most_recent_date] \n",
    "        \n",
    "\n",
    "    print(f\"Last {days_req} days total {len(winners)} products increased rank\")\n",
    "    print(f\"Last {days_req} days total {len(losers)} products lost rank\")\n",
    "    print(f\"Last {days_req} days total {len(neutrals)} products stay neutral\")           \n",
    "\n",
    "    return merged_df,weekly_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example usage: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for periods\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "ma_period = 'your value to evaluate bounds based on moving averages (e.g. 5,7,10,14,30,60,90)'\n",
    "days_req = 'Period that you would like to conduct your analysis to compare with moving averages and bounds (e.g. 5,7,10,14,30,60,90)' \n",
    "\n",
    "#Parameters for bounds thresholds\n",
    "\n",
    "ub = 'insert your bound values (e.g. 0.1, 0.5, 0.8 a value between 0 and 1 would make the bounds tighter or looser based on your choice.)'\n",
    "lb = 'insert your bound values (e.g. 0.1, 0.5, 0.8 a value between 0 and 1 would make the bounds tighter or looser based on your choice.)'\n",
    "\n",
    "sales_rank_report,weekly_report = sales_rank_alert(updated_df,ma_period,days_req,ub,lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sales And Traffic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sales_alert(df,forecast_dir,ub,lb):\n",
    "\n",
    "    forecast_df = pd.read_csv(forecast_dir) \n",
    "    # Clean empty spaces from column names\n",
    "    forecast_df.columns = forecast_df.columns.str.strip()\n",
    "    most_recent_date =  df['date'].max()\n",
    "\n",
    "    start_date = most_recent_date - pd.Timedelta(days = 7)\n",
    "\n",
    "    filtered_df = df[df['date'] > start_date]\n",
    "\n",
    "    merged_df = pd.merge(filtered_df, forecast_df, on ='asin', how = 'left')\n",
    "\n",
    "    grouped_df = merged_df.groupby('asin').agg(\n",
    "        last_sale_date =('date','max'),\n",
    "        total_order_items=('units_ordered', 'sum'),     \n",
    "        weekly_forecast=('forecast', lambda x: round(x.iloc[0] * 7,1)),\n",
    "        price = ('price', 'last')\n",
    "    ).reset_index()\n",
    "\n",
    "    grouped_df['sales_alert'] = ''\n",
    "\n",
    "    for i in range(len(grouped_df)):\n",
    "        if grouped_df['total_order_items'].iloc[i]  < grouped_df['weekly_forecast'].iloc[i] * (1 -lb):\n",
    "            grouped_df.loc[i, 'sales_alert'] = 'Low Sales'\n",
    "        elif grouped_df['total_order_items'].iloc[i]  > grouped_df['weekly_forecast'].iloc[i] * (1 + ub):\n",
    "            grouped_df.loc[i, 'sales_alert'] = 'High Sales'\n",
    "        else:\n",
    "            #grouped_df['Total_Units_Ordered'].iloc[i] > grouped_df['Weekly_Forecast'].iloc[i] * ub and grouped_df['Total_Units_Ordered'].iloc[i] < grouped_df['Weekly_Forecast'].iloc[i] * ub:\n",
    "            grouped_df.loc[i, 'sales_alert'] = 'Neutral'\n",
    "\n",
    "    return grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forecasts as input\n",
    "forecast_dir = 'enter your .csv file with the sales forecasts for the period you are expecting (.csv should consist 2 columns one with asins and another units order forecasts for given period of time)' \n",
    "\n",
    "#Parameters for bounds thresholds\n",
    "\n",
    "ub = 'insert your bound values (e.g. 0.1, 0.5, 0.8 a value between 0 and 1 would make the bounds tighter or looser based on your choice.)'\n",
    "lb = 'insert your bound values (e.g. 0.1, 0.5, 0.8 a value between 0 and 1 would make the bounds tighter or looser based on your choice.)'\n",
    "\n",
    "sales_report = sales_alert(updated_df,forecast_dir,ub,lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inventory Tracker** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inventory tracker is to track any new inventory arrival for the Amazon's FBA warehouses, once the new inventory updated and products have new stocks can be set to live or make adjustments without checking products individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inventory_tracker(df):\n",
    "    selected_columns = ('date', 'asin', 'fba_available')\n",
    "    \n",
    "    restocked_products = []\n",
    "    new_stock_live = []\n",
    "\n",
    "    for sku in df['sku'].unique():\n",
    "        sku_df = df[(df['sku'] == sku) & (df['warehouse_code'] == 'SELLABLE')].reset_index(drop=True)\n",
    "\n",
    "       # Check for new stock live (first entry with Available_(FBA) > 0)\n",
    "        if len(sku_df) == 1 and sku_df['fba_available'].iloc[0] > 0:\n",
    "            new_stock_live.append(sku_df)\n",
    "        \n",
    "        elif len(sku_df) >= 2:\n",
    "            # If there's a new shipment (from 0 to some amount)\n",
    "            if sku_df['fba_available'].iloc[-1] > 0 and sku_df['fba_available'].iloc[-2] == 0:\n",
    "                new_stock_live.append(sku_df)\n",
    "            # Check if there's a restock\n",
    "            elif sku_df['fba_available'].iloc[-1] > sku_df['fba_available'].iloc[-2]:\n",
    "                restocked_products.append(sku_df)\n",
    "\n",
    "   \n",
    "    # Concatenate new stock DataFrames and filter by the latest date\n",
    "    if new_stock_live:\n",
    "        new_stock_df = pd.concat(new_stock_live, ignore_index=True)\n",
    "        new_stock_df = new_stock_df[new_stock_df['date'] == new_stock_df['date'].max()]\n",
    "    else:\n",
    "        new_stock_df = pd.DataFrame(columns=selected_columns)  # Empty DataFrame with selected columns\n",
    "\n",
    "    # Concatenate restocked products and filter by the latest date\n",
    "    if restocked_products:\n",
    "        restocked_df = pd.concat(restocked_products, ignore_index=True)\n",
    "        restocked_df = restocked_df[restocked_df['date'] == restocked_df['date'].max()]\n",
    "    else:\n",
    "        restocked_df = pd.DataFrame(columns=selected_columns)  # Empty DataFrame with selected columns\n",
    "\n",
    "    return new_stock_df, restocked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_dir = \"your directory\"\n",
    "\n",
    "inventory_df = daily_inventory_update(inventory_dir)\n",
    "new_inventory_list, restocked_inventory_list = inventory_tracker(inventory_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART III: Creating a Database and Dashboarding Through PowerBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further all these merged dataframes and performed analysis can be stored in a SQL server to be able to create automated dynamic reports through PowerBI. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connecting to an SQL server and posting the updated dataframes. In order to be able to post, table queries should be performed in SQL server with desired outputs or reports. An example usage can be seen below with PostgreSQL version 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IF THE PASSOWRD HAS @ SPECIAL CHARACTERS TRANSFORM ACCORDINGLY OR USE urllib.parse\n",
    "engine = create_engine('postgresql+psycopg2://servername:yourpassword@localhost/database')\n",
    "\n",
    "#INVENTORY TABLE UPDATE\n",
    "inventory_df.to_sql('tablename', engine, if_exists='append',index =False)\n",
    "\n",
    "#SALESRANK TABLE UPDATE\n",
    "sales_rank_data.to_sql('tablename', engine, if_exists='append',index =False)\n",
    "\n",
    "#SALESTRAFFIC TABLE UPDATE\n",
    "sales_traffic_data.to_sql('tablename', engine, if_exists='append',index =False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the datasets are posted into sql server further data manipulations can be performed by basic sql queries for dashboarding purposes. An example of dashboards for products can be seen in the read.me file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![highsales.png](highsales.png 'yo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lowsales.png](lowsales.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![neutr.png](neutr.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
